#!/usr/bin/env python3
"""
MP CSV â†’ Google Sheets ç›´æ¥æŠ•å…¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (v2)
==============================================
åº—èˆ—ã”ã¨ã«ã‚·ãƒ¼ãƒˆã‚’ä½œæˆã—ã€ãƒãƒ£ãƒãƒ«åˆ¥ãƒ˜ãƒƒãƒ€ãƒ¼+ãƒ•ãƒ©ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿è¡Œã‚’æ›¸ãè¾¼ã‚€ã€‚

Usage:
  python3 import_to_sheets.py

å¯¾è±¡: JW, GA, BG, NP, Ce, RP, BQ, RYB (csv_output/ é…ä¸‹)
èªè¨¼: Service Account or OAuth (gspread + google-auth)
"""

import csv
import os
import sys

try:
    import gspread
    from google.oauth2.service_account import Credentials
except ImportError:
    print("å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("  pip3 install gspread google-auth")
    sys.exit(1)

# â”€â”€ ãƒ‘ã‚¹è¨­å®š â”€â”€
CSV_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'csv_output')
SPREADSHEET_NAME = 'MP_DailySales'

# â”€â”€ èªè¨¼æƒ…å ±ã®ãƒ‘ã‚¹ï¼ˆService Account JSONï¼‰ â”€â”€
CREDS_PATHS = [
    os.path.expanduser('~/dotfiles/SVD_L1_08_Restaurant_Sales/mp_dashboard/gas/credentials.json'),
    os.path.expanduser('~/.config/gspread/service_account.json'),
    os.path.expanduser('~/credentials.json'),
]

# â”€â”€ åº—èˆ—åˆ¥ã‚·ãƒ¼ãƒˆå®šç¾© â”€â”€
# ã‚·ãƒ¼ãƒˆå: ãƒ˜ãƒƒãƒ€ãƒ¼åˆ—ã®å®šç¾©
STORE_SHEETS = {
    'MOIWA_JW': {
        'csv_file': 'JW_daily.csv',
        'headers': ['date', 'Lå£²ä¸Š', 'Läººæ•°', 'Då£²ä¸Š', 'Däººæ•°', 'T.O', 'å¸­æ–™', 'å—äº¬éŒ ', 'èŠ±æŸ', 'ç‰©è²©_é£Ÿå“', 'ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«'],
        'mapper': lambda r: [
            r.get('date', ''),
            i(r, 'l_total'), i(r, 'l_count'),
            i(r, 'd_total'), i(r, 'd_count'),
            i(r, 'to_total'),
            i(r, 'seat_fee'),
            i(r, 'lock_fee'),
            i(r, 'flower'),
            i(r, 'curry'),  # ã‚«ãƒ¬ãƒ¼ â†’ ç‰©è²©_é£Ÿå“
            0,  # ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«ï¼ˆç¾åœ¨ãªã—ï¼‰
        ]
    },
    'TVTOWER_GA': {
        'csv_file': 'GA_daily.csv',
        'headers': ['date', 'Lå£²ä¸Š', 'Läººæ•°', 'Då£²ä¸Š', 'Däººæ•°', 'ATWå£²ä¸Š', 'ATWäººæ•°', 'å®´ä¼šå£²ä¸Š', 'å®´ä¼šäººæ•°', 'å®¤æ–™', 'å±•æœ›å°', 'ç‰©è²©_é£Ÿå“', 'ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«'],
        'mapper': lambda r: [
            r.get('date', ''),
            i(r, 'l_total'), i(r, 'l_count'),
            i(r, 'd_total'), i(r, 'd_count'),
            i(r, 'atw_total'), i(r, 'atw_count'),
            i(r, 'bq_total'), i(r, 'bq_count'),
            i(r, 'room_fee'),
            i(r, 'ticket'),
            0,  # ç‰©è²©_é£Ÿå“ï¼ˆç¾åœ¨ãªã—ï¼‰
            0,  # ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«ï¼ˆç¾åœ¨ãªã—ï¼‰
        ]
    },
    'TVTOWER_BG': {
        'csv_file': 'GA_daily.csv',  # BGãƒ‡ãƒ¼ã‚¿ã¯GA_daily.csvã®bg_*åˆ—ã«ã‚ã‚‹
        'headers': ['date', 'Food', 'Drink', 'Tent', 'äººæ•°', 'ç‰©è²©_é£Ÿå“', 'ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«'],
        'mapper': lambda r: [
            r.get('date', ''),
            i(r, 'bg_food'),
            i(r, 'bg_drink'),
            i(r, 'bg_tent'),
            i(r, 'bg_count'),
            0,  # ç‰©è²©_é£Ÿå“ï¼ˆå°†æ¥ç”¨ï¼‰
            i(r, 'bg_goods'),  # Tã‚·ãƒ£ãƒ„ â†’ ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«
        ],
        'filter': lambda r: i(r, 'bg_total') > 0  # BGç¨¼åƒæ—¥ã®ã¿
    },
    'OKURAYAMA_NP': {
        'csv_file': 'NP_daily.csv',
        'headers': ['date', 'Lå£²ä¸Š', 'Läººæ•°', 'Då£²ä¸Š', 'Däººæ•°', 'å®¤æ–™', 'èŠ±æŸ', 'Eventå£²ä¸Š', 'Eventäººæ•°', 'ç‰©è²©_é£Ÿå“', 'ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«'],
        'mapper': lambda r: [
            r.get('date', ''),
            i(r, 'l_total'), i(r, 'l_count'),
            i(r, 'd_total'), i(r, 'd_count'),
            i(r, 'l_room_fee') + i(r, 'd_room_fee'),  # Lå®¤æ–™+Då®¤æ–™ â†’ å®¤æ–™
            i(r, 'l_flower') + i(r, 'd_flower'),       # LèŠ±æŸ+DèŠ±æŸ â†’ èŠ±æŸ
            i(r, 'event_total'), i(r, 'event_count'),
            0,  # ç‰©è²©_é£Ÿå“ï¼ˆç¾åœ¨ãªã—ï¼‰
            0,  # ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«ï¼ˆç¾åœ¨ãªã—ï¼‰
        ]
    },
    'OKURAYAMA_Ce': {
        'csv_file': 'Ce_daily.csv',
        'headers': ['date', 'æ–™ç†', 'é£²æ–™', 'äººæ•°', 'ç‰©è²©_é£Ÿå“', 'ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«'],
        'mapper': lambda r: [
            r.get('date', ''),
            i(r, 'food'),
            i(r, 'drink'),
            i(r, 'count'),
            0,  # ç‰©è²©_é£Ÿå“ï¼ˆå°†æ¥: goodsã®é£Ÿå“åˆ†ï¼‰
            i(r, 'goods'),  # æš«å®š: goodsã‚’ã‚¢ãƒ‘ãƒ¬ãƒ«ã¸
        ]
    },
    'OKURAYAMA_RP': {
        'csv_file': 'RP_daily.csv',
        'headers': ['date', 'æ–™ç†', 'é£²æ–™', 'äººæ•°', 'ç‰©è²©_é£Ÿå“', 'ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«'],
        'mapper': lambda r: [
            r.get('date', ''),
            i(r, 'food'),
            i(r, 'drink'),
            i(r, 'count'),
            0,  # ç‰©è²©_é£Ÿå“ï¼ˆå°†æ¥: goodsã®é£Ÿå“åˆ†ï¼‰
            i(r, 'goods'),  # æš«å®š: goodsã‚’ã‚¢ãƒ‘ãƒ¬ãƒ«ã¸
        ]
    },
    'AKARENGA_BQ': {
        'csv_file': 'BQ_daily.csv',
        'headers': ['date', 'Lå£²ä¸Š', 'Läººæ•°', 'ATå£²ä¸Š', 'ATäººæ•°', 'Då£²ä¸Š', 'Däººæ•°', 'å¸­æ–™', 'ç‰©è²©_é£Ÿå“', 'ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«'],
        'mapper': lambda r: [
            r.get('date', ''),
            i(r, 'l_total'), i(r, 'l_count'),
            i(r, 'at_total'), i(r, 'at_count'),
            i(r, 'd_total'), i(r, 'd_count'),
            i(r, 'seat_fee'),
            0,  # ç‰©è²©_é£Ÿå“ï¼ˆç¾åœ¨ãªã—ï¼‰
            0,  # ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«ï¼ˆç¾åœ¨ãªã—ï¼‰
        ]
    },
    'AKARENGA_RYB': {
        'csv_file': 'BQ_daily.csv',  # RYBãƒ‡ãƒ¼ã‚¿ã¯BQ_daily.csvã®ryb_*åˆ—ã«ã‚ã‚‹
        'headers': ['date', 'Food', 'Drink', 'äººæ•°', 'ç‰©è²©_é£Ÿå“', 'ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«'],
        'mapper': lambda r: [
            r.get('date', ''),
            i(r, 'ryb_food'),
            i(r, 'ryb_drink'),
            i(r, 'ryb_count'),
            0,  # ç‰©è²©_é£Ÿå“ï¼ˆç¾åœ¨ãªã—ï¼‰
            0,  # ç‰©è²©_ã‚¢ãƒ‘ãƒ¬ãƒ«ï¼ˆç¾åœ¨ãªã—ï¼‰
        ],
        'filter': lambda r: i(r, 'ryb_total') > 0  # RYBç¨¼åƒæ—¥ã®ã¿
    },
}


def i(row, key):
    """Safe int extraction"""
    val = row.get(key, '')
    if val == '' or val is None:
        return 0
    try:
        return int(float(val))
    except (ValueError, TypeError):
        return 0


def find_credentials():
    """èªè¨¼æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™"""
    for p in CREDS_PATHS:
        if os.path.exists(p):
            return p
    return None


def connect_spreadsheet():
    """Google Sheetsã«æ¥ç¶š"""
    creds_path = find_credentials()
    if not creds_path:
        print("âŒ èªè¨¼æƒ…å ±(credentials.json)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        print("ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã«é…ç½®ã—ã¦ãã ã•ã„:")
        for p in CREDS_PATHS:
            print(f"  - {p}")
        sys.exit(1)

    print(f"ğŸ”‘ èªè¨¼: {creds_path}")
    scopes = [
        'https://www.googleapis.com/auth/spreadsheets',
        'https://www.googleapis.com/auth/drive'
    ]
    creds = Credentials.from_service_account_file(creds_path, scopes=scopes)
    gc = gspread.authorize(creds)

    try:
        ss = gc.open(SPREADSHEET_NAME)
        print(f"ğŸ“Š ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ: {ss.title} ({ss.url})")
    except gspread.SpreadsheetNotFound:
        print(f"âŒ '{SPREADSHEET_NAME}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Service Accountã«å…±æœ‰ã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    return ss


def process_store(ss, sheet_name, config):
    """1åº—èˆ—åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€"""
    csv_path = os.path.join(CSV_DIR, config['csv_file'])
    if not os.path.exists(csv_path):
        print(f"  âš  {csv_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
        return 0

    # CSVãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    records = []
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if not row.get('date', '').strip():
                continue
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ï¼ˆBG/RYBã¯ç¨¼åƒæ—¥ã®ã¿ï¼‰
            if 'filter' in config and not config['filter'](row):
                continue
            try:
                mapped = config['mapper'](row)
                records.append(mapped)
            except Exception as e:
                print(f"  âš  ãƒãƒƒãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({row.get('date', '?')}): {e}")

    if not records:
        print(f"  âš  ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
        return 0

    # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
    records.sort(key=lambda r: r[0])

    # ã‚·ãƒ¼ãƒˆå–å¾— or æ–°è¦ä½œæˆ
    try:
        ws = ss.worksheet(sheet_name)
        print(f"  ğŸ“‹ æ—¢å­˜ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã‚’ã‚¯ãƒªã‚¢")
        ws.clear()
    except gspread.WorksheetNotFound:
        ws = ss.add_worksheet(title=sheet_name, rows=len(records) + 10, cols=len(config['headers']))
        print(f"  ğŸ“‹ æ–°è¦ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã‚’ä½œæˆ")

    # ãƒ˜ãƒƒãƒ€ãƒ¼ + ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬æ›¸ãè¾¼ã¿
    all_data = [config['headers']] + records
    ws.update(range_name='A1', values=all_data)

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’å¤ªå­—+ãƒ•ãƒªãƒ¼ã‚º
    ws.format('1:1', {'textFormat': {'bold': True}})
    ws.freeze(rows=1)

    print(f"  âœ… {len(records)} è¡Œã‚’æ›¸ãè¾¼ã¿å®Œäº†")
    return len(records)


def main():
    import argparse
    parser = argparse.ArgumentParser(description='MP CSV â†’ Google Sheets ç›´æ¥æŠ•å…¥')
    parser.add_argument('--store', help='ç‰¹å®šåº—èˆ—ã®ã¿ (ä¾‹: MOIWA_JW)')
    parser.add_argument('--dry-run', action='store_true', help='æ¥ç¶šãƒ†ã‚¹ãƒˆã®ã¿')
    args = parser.parse_args()

    print('â•' * 60)
    print('MOMENTUM PEAKS â€” CSV â†’ Google Sheets ç›´æ¥æŠ•å…¥ v2')
    print('â•' * 60)

    ss = connect_spreadsheet()
    grand_total = 0

    stores = {args.store: STORE_SHEETS[args.store]} if args.store else STORE_SHEETS

    for sheet_name, config in stores.items():
        print(f'\nğŸ“‚ {sheet_name} ({config["csv_file"]}) ...')

        if args.dry_run:
            csv_path = os.path.join(CSV_DIR, config['csv_file'])
            if os.path.exists(csv_path):
                with open(csv_path, 'r', encoding='utf-8') as f:
                    count = sum(1 for _ in csv.DictReader(f))
                print(f'  [DRY RUN] {count} è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š')
            else:
                print(f'  [DRY RUN] CSVãªã—')
            continue

        count = process_store(ss, sheet_name, config)
        grand_total += count

    print(f'\n{"â•" * 60}')
    print(f'åˆè¨ˆ: {grand_total} è¡Œ / {len(stores)} ã‚·ãƒ¼ãƒˆ')
    print('â•' * 60)


if __name__ == '__main__':
    main()
