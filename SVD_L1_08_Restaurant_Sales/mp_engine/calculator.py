#!/usr/bin/env python3
"""
Momentum Peaks â€” MPè¨ˆç®—ã‚³ã‚¢ (calculator.py)
å…¨åº—èˆ— Ã— å…¨æ—¥ä»˜ã®MPãƒã‚¤ãƒ³ãƒˆã‚’è‡ªå‹•è¨ˆç®—ã™ã‚‹æ±ç”¨ã‚¨ãƒ³ã‚¸ãƒ³ã€‚

Architecture:
  Config Layer  â†’  restaurant_config.json (é¡§å®¢ã”ã¨ã«å·®ã—æ›¿ãˆ)
  Indices Layer â†’  mp_indices.json (æ‹ ç‚¹å®šæŒ‡æ•°)
  Sekki Engine  â†’  sekki.py (æ—¥ä»˜â†’ç¯€æ°—â†’å­£ç¯€PT)
  Data Layer    â†’  å„åº—èˆ—CSV (æ—¥æ¬¡å£²ä¸Šãƒ‡ãƒ¼ã‚¿)
  Output        â†’  {store}_mp_daily.csv (æ—¥æ¬¡MPãƒã‚¤ãƒ³ãƒˆ)
"""

import csv
import json
import os
import sys
from datetime import date, timedelta
from typing import Dict, List, Optional, Tuple
from collections import defaultdict

# åŒä¸€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¯€æ°—ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from sekki import get_sekki, get_weekday_ja, get_kf1, _load_indices


class MPCalculator:
    """Momentum Peaks è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, engine_dir: str = None):
        if engine_dir is None:
            engine_dir = os.path.dirname(os.path.abspath(__file__))
        
        self.engine_dir = engine_dir
        self.csv_dir = os.path.join(os.path.dirname(engine_dir), "csv_output")
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        with open(os.path.join(engine_dir, "restaurant_config.json"), "r", encoding="utf-8") as f:
            self.config = json.load(f)
        
        self.indices = _load_indices()
        
        # å…¨åº—èˆ—ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
        self.store_data = {}
        self._load_all_store_data()
    
    def _load_all_store_data(self):
        """å…¨åº—èˆ—ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        for base in self.config["bases"]:
            for store in base["stores"]:
                store_id = store["id"]
                csv_path = os.path.join(self.csv_dir, store["csv_file"])
                if not os.path.exists(csv_path):
                    print(f"âš ï¸  CSV not found: {csv_path} (store: {store_id})")
                    continue
                
                # KFè¨ˆç®—ã‹ã‚‰é™¤å¤–ã™ã‚‹ãƒãƒ£ãƒãƒ«
                exclude_channels = set(store.get("exclude_channels_from_kf", []))
                
                data = {}
                with open(csv_path, "r", encoding="utf-8") as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        d = row["date"]
                        grand_total_col = store["grand_total_col"]
                        
                        # æ¥å®¢æ•°ã®è¨ˆç®—ï¼ˆãƒãƒ£ãƒãƒ«åˆ¥countåˆ—ã®åˆè¨ˆï¼‰
                        total_count = 0
                        excluded_count = 0
                        excluded_sales = 0
                        for ch_name, ch_conf in store["channels"].items():
                            count_val = 0
                            sales_val = 0
                            if "count_col" in ch_conf and ch_conf["count_col"] in row:
                                try:
                                    count_val = int(row[ch_conf["count_col"]] or 0)
                                except (ValueError, KeyError):
                                    pass
                            if "sales_col" in ch_conf and ch_conf["sales_col"] in row:
                                try:
                                    sales_val = int(row[ch_conf["sales_col"]] or 0)
                                except (ValueError, KeyError):
                                    pass
                            
                            total_count += count_val
                            
                            if ch_name in exclude_channels:
                                excluded_count += count_val
                                excluded_sales += sales_val
                        
                        # å£²ä¸Šé¡
                        try:
                            grand_total = int(row.get(grand_total_col, 0) or 0)
                        except ValueError:
                            grand_total = 0
                        
                        # KFè¨ˆç®—ç”¨ï¼ˆé™¤å¤–ãƒãƒ£ãƒãƒ«åˆ†ã‚’å·®ã—å¼•ãï¼‰
                        kf_sales = grand_total - excluded_sales
                        kf_count = total_count - excluded_count
                        
                        data[d] = {
                            "total_count": total_count,
                            "grand_total": grand_total,
                            "kf_sales": kf_sales,
                            "kf_count": kf_count,
                            "weekday": row.get("weekday", ""),
                        }
                
                self.store_data[store_id] = data
                if exclude_channels:
                    print(f"âœ… Loaded {store_id}: {len(data)} days (KFé™¤å¤–: {', '.join(exclude_channels)})")
                else:
                    print(f"âœ… Loaded {store_id}: {len(data)} days")
    
    def _get_base_id_for_store(self, store_id: str) -> str:
        """åº—èˆ—IDã‹ã‚‰æ‹ ç‚¹IDã‚’é€†å¼•ã"""
        for base in self.config["bases"]:
            for store in base["stores"]:
                if store["id"] == store_id:
                    return base["id"]
        raise ValueError(f"Store '{store_id}' not found in config")
    
    def calculate_kf2_kf3(self, store_id: str, target_date: date,
                          lookback_years: int = 2) -> Tuple[float, float]:
        """
        KFâ‘¡ï¼ˆå£²ä¸Šãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼‰ã¨KFâ‘¢ï¼ˆæ¥å®¢æ•°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚
        éå»Nå¹´ã®åŒæœˆå®Ÿç¸¾ã‹ã‚‰æ­£è¦åŒ–ï¼ˆmin-max â†’ 1.00-5.00ï¼‰ã€‚
        
        Args:
            store_id: åº—èˆ—ID
            target_date: å¯¾è±¡æ—¥ä»˜
            lookback_years: å‚ç…§ã™ã‚‹éå»å¹´æ•°
        
        Returns:
            (kf2, kf3) as floats
        """
        if store_id not in self.store_data:
            return 2.50, 2.50  # ãƒ‡ãƒ¼ã‚¿ãªã—ã®å ´åˆã¯ä¸­é–“å€¤
        
        data = self.store_data[store_id]
        target_month = target_date.month
        
        # éå»ã®åŒæœˆãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆæœˆåˆ¥ã®æ—¥å¹³å‡å£²ä¸Šãƒ»æ—¥å¹³å‡æ¥å®¢æ•°ï¼‰
        monthly_sales = {}  # {year-month: avg_daily_sales}
        monthly_counts = {}  # {year-month: avg_daily_count}
        
        for date_str, row in data.items():
            try:
                d = date.fromisoformat(date_str)
            except ValueError:
                continue
            
            key = f"{d.year}-{d.month:02d}"
            if key not in monthly_sales:
                monthly_sales[key] = []
                monthly_counts[key] = []
            
            if row["kf_sales"] > 0:  # å–¶æ¥­æ—¥ã®ã¿ï¼ˆKFç”¨å£²ä¸Šã§åˆ¤å®šï¼‰
                monthly_sales[key].append(row["kf_sales"])
                monthly_counts[key].append(row["kf_count"])
        
        # æœˆåˆ¥å¹³å‡ã‚’è¨ˆç®—
        month_avg_sales = {}
        month_avg_counts = {}
        for key, values in monthly_sales.items():
            if values:
                month_avg_sales[key] = sum(values) / len(values)
        for key, values in monthly_counts.items():
            if values:
                month_avg_counts[key] = sum(values) / len(values)
        
        if not month_avg_sales:
            return 2.50, 2.50
        
        # å…¨æœˆã®å¹³å‡å€¤ã§min-maxæ­£è¦åŒ–ã®åŸºæº–ã‚’ä½œã‚‹
        all_sales_avgs = list(month_avg_sales.values())
        all_count_avgs = list(month_avg_counts.values())
        
        min_sales = min(all_sales_avgs)
        max_sales = max(all_sales_avgs)
        min_count = min(all_count_avgs) if all_count_avgs else 0
        max_count = max(all_count_avgs) if all_count_avgs else 1
        
        # å¯¾è±¡æœˆã®éå»Nå¹´ã®å¹³å‡ã‚’å–å¾—
        target_sales_values = []
        target_count_values = []
        
        for y in range(target_date.year - lookback_years, target_date.year + 1):
            key = f"{y}-{target_month:02d}"
            if key in month_avg_sales:
                target_sales_values.append(month_avg_sales[key])
            if key in month_avg_counts:
                target_count_values.append(month_avg_counts[key])
        
        # éå»Nå¹´ã®åŒæœˆå¹³å‡
        if target_sales_values:
            avg_sales = sum(target_sales_values) / len(target_sales_values)
        else:
            avg_sales = sum(all_sales_avgs) / len(all_sales_avgs)
        
        if target_count_values:
            avg_count = sum(target_count_values) / len(target_count_values)
        else:
            avg_count = sum(all_count_avgs) / len(all_count_avgs) if all_count_avgs else 0
        
        # min-maxæ­£è¦åŒ– â†’ 1.00-5.00
        def normalize(val, vmin, vmax):
            if vmax == vmin:
                return 3.00
            normalized = (val - vmin) / (vmax - vmin)
            return round(1.00 + normalized * 4.00, 2)
        
        kf2 = normalize(avg_sales, min_sales, max_sales)
        kf3 = normalize(avg_count, min_count, max_count)
        
        return kf2, kf3
    
    def calculate_mp_point(self, store_id: str, target_date: date) -> Dict:
        """
        1æ—¥åˆ†ã®MPãƒã‚¤ãƒ³ãƒˆã‚’è¨ˆç®—ã™ã‚‹ã€‚
        
        Formula: MP Point = (KFâ‘  + KFâ‘¡ + KFâ‘¢) / 3
        
        Returns:
            Dict with all calculation details
        """
        base_id = self._get_base_id_for_store(store_id)
        
        # KFâ‘ : æ‹ ç‚¹å®šæŒ‡æ•°
        kf1_result = get_kf1(target_date, base_id, self.indices)
        kf1 = kf1_result["kf1"]
        
        # KFâ‘¡â‘¢: åº—èˆ—å®Ÿç¸¾ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
        kf2, kf3 = self.calculate_kf2_kf3(store_id, target_date)
        
        # æœ€çµ‚MPãƒã‚¤ãƒ³ãƒˆ
        mp_point = round((kf1 + kf2 + kf3) / 3, 2)
        
        # äºˆæ¸¬å€¤ã®è¨ˆç®—ï¼ˆéå»åŒæœˆåŒæ›œæ—¥ã®å¹³å‡ï¼‰
        predicted_sales, predicted_count = self._predict(store_id, target_date)
        
        # å®Ÿç¸¾å€¤ã®å–å¾—
        actual = self.store_data.get(store_id, {}).get(target_date.isoformat(), {})
        
        return {
            "date": target_date.isoformat(),
            "weekday": kf1_result["weekday_ja"],
            "sekki": kf1_result["sekki"],
            "rank": kf1_result["rank"],
            "season": kf1_result["season"],
            "season_pt": kf1_result["season_pt"],
            "seasonal_idx": kf1_result["seasonal_idx"],
            "weekday_idx": kf1_result["weekday_idx"],
            "visitor_idx": kf1_result["visitor_idx"],
            "kf1": kf1,
            "kf2": kf2,
            "kf3": kf3,
            "mp_point": mp_point,
            "predicted_sales": predicted_sales,
            "predicted_count": predicted_count,
            "actual_sales": actual.get("grand_total", 0),
            "actual_count": actual.get("total_count", 0),
        }
    
    def _predict(self, store_id: str, target_date: date) -> Tuple[int, int]:
        """éå»åŒæœˆåŒæ›œæ—¥ã®åŠ é‡å¹³å‡ã‹ã‚‰äºˆæ¸¬å€¤ã‚’ç®—å‡º"""
        if store_id not in self.store_data:
            return 0, 0
        
        data = self.store_data[store_id]
        target_weekday = target_date.weekday()
        target_month = target_date.month
        
        sales_values = []
        count_values = []
        weights = []
        
        for date_str, row in data.items():
            try:
                d = date.fromisoformat(date_str)
            except ValueError:
                continue
            
            # åŒæœˆãƒ»åŒæ›œæ—¥ã®ã¿
            if d.month == target_month and d.weekday() == target_weekday and d < target_date:
                if row["grand_total"] > 0:
                    # æ–°ã—ã„å¹´ã®ãƒ‡ãƒ¼ã‚¿ã«é«˜ã„é‡ã¿ã‚’ä»˜ã‘ã‚‹
                    year_diff = target_date.year - d.year
                    weight = 1.0 / max(year_diff, 1)
                    sales_values.append(row["grand_total"])
                    count_values.append(row["total_count"])
                    weights.append(weight)
        
        if not sales_values:
            return 0, 0
        
        # åŠ é‡å¹³å‡
        total_weight = sum(weights)
        pred_sales = int(sum(s * w for s, w in zip(sales_values, weights)) / total_weight)
        pred_count = int(sum(c * w for c, w in zip(count_values, weights)) / total_weight)
        
        return pred_sales, pred_count
    
    def generate_mp_csv(self, store_id: str, 
                        start_date: date, end_date: date,
                        output_path: str = None) -> str:
        """
        æŒ‡å®šæœŸé–“ã®MP CSVã‚’ç”Ÿæˆã™ã‚‹ã€‚
        
        Args:
            store_id: åº—èˆ—ID
            start_date: é–‹å§‹æ—¥
            end_date: çµ‚äº†æ—¥
            output_path: å‡ºåŠ›ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯csv_output/{store}_mp_daily.csvï¼‰
        
        Returns:
            å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        if output_path is None:
            output_path = os.path.join(self.csv_dir, f"{store_id}_mp_daily.csv")
        
        fieldnames = [
            "date", "weekday", "sekki", "rank", "season", "season_pt",
            "seasonal_idx", "weekday_idx", "visitor_idx",
            "kf1", "kf2", "kf3", "mp_point",
            "predicted_sales", "predicted_count",
            "actual_sales", "actual_count",
        ]
        
        rows = []
        current = start_date
        total_days = (end_date - start_date).days + 1
        
        print(f"\nğŸ”„ Calculating MP for {store_id}: {start_date} â†’ {end_date} ({total_days} days)")
        
        while current <= end_date:
            try:
                row = self.calculate_mp_point(store_id, current)
                rows.append(row)
            except Exception as e:
                print(f"  âš ï¸  Error on {current}: {e}")
            
            current += timedelta(days=1)
            
            # é€²æ—è¡¨ç¤º
            done = len(rows)
            if done % 100 == 0:
                print(f"  ğŸ“Š {done}/{total_days} days calculated...")
        
        with open(output_path, "w", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)
        
        print(f"âœ… Generated: {output_path} ({len(rows)} rows)")
        return output_path


def main():
    """CLI: å…¨åº—èˆ—ã®MP CSVã‚’ç”Ÿæˆ"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Momentum Peaks Calculator")
    parser.add_argument("--store", type=str, default=None, 
                        help="Store ID (e.g., JW, NP, GA, BQ). Default: all stores")
    parser.add_argument("--start", type=str, default="2023-04-01",
                        help="Start date (YYYY-MM-DD)")
    parser.add_argument("--end", type=str, default="2026-03-31",
                        help="End date (YYYY-MM-DD)")
    parser.add_argument("--verify", action="store_true",
                        help="Run verification against existing MP CSVs")
    parser.add_argument("--single", type=str, default=None,
                        help="Calculate single date (e.g., 2023-09-07)")
    
    args = parser.parse_args()
    
    calc = MPCalculator()
    
    if args.single:
        target = date.fromisoformat(args.single)
        store = args.store or "JW"
        result = calc.calculate_mp_point(store, target)
        
        print(f"\n{'â”' * 50}")
        print(f"ğŸ“… {result['date']} ({result['weekday']})")
        print(f"ğŸŒ¿ {result['sekki']} (Level {result['rank']}) â€” {result['season']}")
        print(f"{'â”€' * 50}")
        print(f"â‘  å­£ç¯€IDX:    {result['seasonal_idx']:.2f}")
        print(f"â‘¡ æ›œæ—¥IDX:    {result['weekday_idx']:.2f}")
        print(f"â‘¢ æ¥å ´è€…IDX:  {result['visitor_idx']:.2f}")
        print(f"   KFâ‘  =      {result['kf1']:.2f}")
        print(f"{'â”€' * 50}")
        print(f"â‘¤ KFâ‘¡ (å£²ä¸Š): {result['kf2']:.2f}")
        print(f"â‘¥ KFâ‘¢ (æ¥å®¢): {result['kf3']:.2f}")
        print(f"{'â”' * 50}")
        print(f"ğŸ”¥ MP Point =  {result['mp_point']:.2f}")
        print(f"{'â”' * 50}")
        print(f"ğŸ“ˆ äºˆæ¸¬å£²ä¸Š:   Â¥{result['predicted_sales']:,}")
        print(f"ğŸ‘¥ äºˆæ¸¬æ¥å®¢:   {result['predicted_count']}å")
        print(f"ğŸ“Š å®Ÿç¸¾å£²ä¸Š:   Â¥{result['actual_sales']:,}")
        print(f"ğŸ‘¤ å®Ÿç¸¾æ¥å®¢:   {result['actual_count']}å")
        return
    
    if args.verify:
        _verify(calc, args.store or "JW")
        return
    
    start = date.fromisoformat(args.start)
    end = date.fromisoformat(args.end)
    
    # å¯¾è±¡åº—èˆ—
    if args.store:
        stores = [args.store]
    else:
        stores = []
        for base in calc.config["bases"]:
            for store in base["stores"]:
                stores.append(store["id"])
    
    for store_id in stores:
        if store_id in calc.store_data:
            calc.generate_mp_csv(store_id, start, end)
        else:
            print(f"âš ï¸  Skipping {store_id}: no data loaded")


def _verify(calc, store_id: str):
    """æ—¢å­˜ã®MP CSVã¨ç…§åˆæ¤œè¨¼"""
    existing_path = os.path.join(calc.csv_dir, f"{store_id}_mp_24levels.csv")
    if not os.path.exists(existing_path):
        print(f"âŒ Existing MP CSV not found: {existing_path}")
        return
    
    print(f"\nğŸ” Verifying {store_id} against {existing_path}")
    
    with open(existing_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        mismatches = 0
        total = 0
        for row in reader:
            total += 1
            d = date.fromisoformat(row["date"])
            
            try:
                result = calc.calculate_mp_point(store_id, d)
            except Exception as e:
                continue
            
            # ç¯€æ°—ã®ä¸€è‡´ã‚’ç¢ºèª
            if result["sekki"] != row["sekki"]:
                print(f"  âš ï¸  {row['date']}: sekki mismatch â€” "
                      f"expected '{row['sekki']}', got '{result['sekki']}'")
                mismatches += 1
            
            if total <= 5 or total % 200 == 0:
                print(f"  âœ… {row['date']}: {result['sekki']} "
                      f"KFâ‘ ={result['kf1']:.2f} MP={result['mp_point']:.2f}")
    
    print(f"\nğŸ“Š Verification: {total} rows checked, {mismatches} mismatches")


if __name__ == "__main__":
    main()
