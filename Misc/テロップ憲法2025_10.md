## 優先順位

- **音声優先:** 文字起こしの内容は、音声データを最優先の正とします。
- **PDF参照:** PDF原稿は、誤字脱字の修正や、話者の意図をより正確に反映させるための参考資料として活用します。

# テロップ憲法2025/10

## 目的

最高のパートナーであるSATと共に、最高品質のテロップを安定して生成するための基準とワークフローを定める。

## 基本原則

- **自然な可読性:** 機械的な改行ではなく、人間が最も自然に感じる「意味のまとまり」と「会話のリズム」を最優先する。
- **文字数:** 1行あたり28文字前後を目安とするが、自然な区切りを優先するため、最大35文字程度までは許容する。
- **表記統一:** 特定の単語は、定められた表記に統一する。

## ワークフロー

1.  **入力:** Whisperによって生成された、単語ごとのタイムスタンプ情報を含むJSONファイル。

2.  **処理:** 以下のPythonスクリプト `reformat_srt_final_v10.py` を使用して、JSONファイルをSRTファイルに変換する。

3.  **出力:** 上記スクリプトによって生成された、フォーマット済みのSRTファイル。

## 改正履歴と教訓

### 2025/10/27

- **事象:** `rie1027.srt` の作成において、PDF原稿を元にした逐次修正を試みた結果、`replace`ツールの `old_string` 不一致エラーが多発。最終的に、音声データを絶対的な正とし、句読点を一括除去する方針へと切り替えた。
- **教訓:**
    1.  **音声こそが絶対的正義:** 文字起こしの正確性は、何よりもまず音声データに依存する。PDF等の参考資料は、あくまで補助的な役割に留めるべきである。
    2.  **修正作業の効率化:** 細かい修正を繰り返すよりも、全体の方針を固め、スクリプトによる一括処理を優先する方が、結果的に速く、正確である。
    3.  **柔軟な方針転換:** 当初の計画に固執せず、問題が発生した際には、より効率的で確実な方法へと、迅速に方針を転換することが、最高のチームワークの鍵である。
- **新ワークフロー:**
    1.  `whisper` で音声からJSONを生成。
    2.  `reformat_srt_final_v9.py` でSRTを生成。
    3.  `remove_punctuation.py` で全ての句読点を除去。
    4.  最終的に目視で、音声と明らかに異なる単語や、意味が通らない箇所のみを最小限の修正で対応する。

reformat_srt_final_v10.py

```python
import json
import re
import sys
from datetime import timedelta

# --- 設定項目 ---
TARGET_CHARS = 28
MAX_CHARS = 35 # これを多少超えることは許容

def format_srt_time(seconds):
    if seconds < 0:
        seconds = 0
    delta = timedelta(seconds=seconds)
    hours, remainder = divmod(delta.total_seconds(), 3600)
    minutes, seconds = divmod(remainder, 60)
    milliseconds = int((seconds - int(seconds)) * 1000)
    return f"{int(hours):02}:{int(minutes):02}:{int(seconds):02},{milliseconds:03d}"

def find_best_break_point(words, start_index):
    """最適な改行ポイントを見つける"""
    current_chars = 0
    last_punctuation_break = -1
    last_particle_break = -1

    for i in range(start_index, len(words)):
        word_info = words[i]
        word = word_info.get('word', '').strip()
        current_chars += len(word)

        if current_chars > MAX_CHARS:
            if last_punctuation_break != -1:
                return last_punctuation_break
            if last_particle_break != -1:
                return last_particle_break
            return i # 強制的に改行

        if re.search(r'[。！？、]', word):
            last_punctuation_break = i + 1
        
        particles = ['は', 'も', 'が', 'に', 'を', 'で', 'と', 'ので', 'から']
        if any(word.endswith(p) for p in particles) and current_chars > 10:
             last_particle_break = i + 1

        if current_chars >= TARGET_CHARS:
            if last_punctuation_break != -1:
                return last_punctuation_break
            if last_particle_break != -1:
                return last_particle_break

    return len(words) # 最後まで

def create_final_subtitles(input_path, output_path):
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"エラー: {e}")
        return

    all_words = [word for segment in data.get('segments', []) for word in segment.get('words', [])]
    if not all_words:
        all_words = data.get('words', [])
    if not all_words:
        print("警告: JSONファイルに単語データが含まれていません。")
        return

    subtitles = []
    current_index = 0
    while current_index < len(all_words):
        break_point = find_best_break_point(all_words, current_index)
        if break_point <= current_index:
            break_point = current_index + 1 # 進行しない場合、強制的に1つ進める
            
        line_words = all_words[current_index:break_point]
        if not line_words:
            break

        start_time = line_words[0]['start']
        end_time = line_words[-1]['end']
        text = "".join([w['word'] for w in line_words]).strip()
        
        if len(text) <= 2 and subtitles:
            prev_sub = subtitles[-1]
            prev_sub['end'] = end_time
            prev_sub['text'] += text
        else:
            subtitles.append({'start': start_time, 'end': end_time, 'text': text})
        
        current_index = break_point

    with open(output_path, 'w', encoding='utf-8') as f:
        for i, sub in enumerate(subtitles, 1):
            start_ts = format_srt_time(sub['start'])
            end_ts = format_srt_time(sub['end'])
            text = sub['text'].strip()
            text = re.sub(r'[、。]', '', text)
            text = text.replace('子供', '子ども')
            text = text.replace('リー', 'りえ')

            if text:
                f.write(f"{i}\n{start_ts} --> {end_ts}\n{text}\n\n")

    print(f"最終版SRTファイルの生成が完了しました: {output_path}")

if __name__ == '__main__':
    if len(sys.argv) != 3:
        print(f"使用法: python {sys.argv[0]} <入力JSON> <出力SRT>")
        sys.exit(1)
    create_final_subtitles(sys.argv[1], sys.argv[2])
```